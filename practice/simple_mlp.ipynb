{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F \n",
    "from torch import nn, optim\n",
    "from sys import exit as e\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(\"./data\", train=True, download=True, transform=trans)\n",
    "test_dataset = MNIST(\"./data\", train=False, download=True, transform=trans)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f96b727a450>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203530\n"
     ]
    }
   ],
   "source": [
    "n_dim = 784 \n",
    "n_hidden1 = 256 \n",
    "n_hidden2 = 256 \n",
    "n_output = 10\n",
    "device = torch.device('cuda')\n",
    "W1 = torch.randn((n_dim, n_hidden1), device=device) / n_dim**0.5 \n",
    "b1 = torch.randn((n_hidden1), device=device)\n",
    "W2 = torch.randn((n_hidden1, n_output), device=device)\n",
    "b2 = torch.randn((n_output), device=device)\n",
    "\n",
    "W1 *= 2**0.5\n",
    "parameters = [W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "  p.requires_grad=True\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20: 0.5183426141738892\n",
      "10/20: 0.23148706555366516\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "with torch.no_grad():\n",
    "  for epoch in range(n_epochs):\n",
    "    for idx, (x, label) in enumerate(train_loader):\n",
    "      x = x.to(device)\n",
    "      label = label.to(device)\n",
    "\n",
    "      x = x.flatten(start_dim=1)\n",
    "      h1 = x @ W1 + b1 \n",
    "      h1R = torch.relu(h1)\n",
    "      logits = h1R @ W2 + b2 \n",
    "\n",
    "      loss = F.cross_entropy(logits, label)\n",
    "\n",
    "      # for p in parameters:\n",
    "      #   p.grad = None\n",
    "\n",
    "      # backprop\n",
    "      dlogits = F.softmax(logits, dim=-1)\n",
    "      dlogits[range(batch_size), label] -= 1 \n",
    "      dlogits /= batch_size\n",
    "\n",
    "      dh1R = dlogits @ W2.T\n",
    "      dW2 = h1R.T @ dlogits\n",
    "      db2 = dlogits.sum(0)\n",
    "\n",
    "      dh1 = torch.zeros_like(h1)\n",
    "      dh1[h1 > 0]  = 1\n",
    "      dh1  = dh1 * dh1R\n",
    "\n",
    "      dW1 = x.T @ dh1\n",
    "      db1 = dh1.sum(0)\n",
    "      grads = [dW1, db1, dW2, db2]\n",
    "\n",
    "      lr = 0.001 if epoch < 100000 else 0.01 # step learning rate decay\n",
    "      for p, grad in zip(parameters, grads):\n",
    "        p.data += -lr * grad\n",
    "\n",
    "      # all_tensors = [logprobs, probs, logits_exp_sum, logits_exp, logits_main, logits_max, logits, h1R, W2, b2, h1, W1,b1]\n",
    "      # for t in all_tensors:\n",
    "        # t.retain_grad()\n",
    "      # loss2 = F.cross_entropy(logits, label)\n",
    "      # loss2.backward()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"{epoch}/{n_epochs}: {loss.item()}\")\n",
    "\n",
    "  #   break\n",
    "  # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameters, grads):\n\u001b[1;32m     19\u001b[0m   p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m*\u001b[39m grad\n\u001b[0;32m---> 21\u001b[0m \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m cmp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1R\u001b[39m\u001b[38;5;124m'\u001b[39m, dh1R, h1R)\n\u001b[1;32m     23\u001b[0m cmp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, dW2, W2)\n",
      "Cell \u001b[0;32mIn[171], line 3\u001b[0m, in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[0;32m----> 3\u001b[0m   ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m   app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      5\u001b[0m   maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "dlogits = F.softmax(logits, dim=-1)\n",
    "dlogits[range(batch_size), label] -= 1 \n",
    "dlogits /= batch_size\n",
    "\n",
    "dh1R = dlogits @ W2.T\n",
    "dW2 = h1R.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "dh1 = torch.zeros_like(h1)\n",
    "dh1[h1 > 0]  = 1\n",
    "dh1  = dh1 * dh1R\n",
    "\n",
    "dW1 = x.T @ dh1\n",
    "db1 = dh1.sum(0)\n",
    "grads = [dW1, db1, dW2, db2]\n",
    "\n",
    "lr = 0.1 if epoch < 100000 else 0.01 # step learning rate decay\n",
    "for p, grad in zip(parameters, grads):\n",
    "  p.data += -lr * grad\n",
    "\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h1R', dh1R, h1R)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "cmp('h1', dh1, h1)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256]),\n",
       " torch.Size([784, 256]),\n",
       " torch.Size([256]),\n",
       " torch.Size([32, 256]),\n",
       " torch.Size([32, 784]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1.shape, W1.shape, b1.shape, dh1.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  for x, label in val_loader:\n",
    "    x = x.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    x = x.flatten(start_dim=1)\n",
    "    h1 = x @ W1 + b1 \n",
    "    h1R = torch.relu(h1)\n",
    "    logits = h1R @ W2 + b2 \n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    y_preds = torch.argmax(probs, dim=-1).detach().tolist()\n",
    "    y_true = label.detach().tolist()\n",
    "\n",
    "    print(accuracy_score(y_preds, y_true))\n",
    "    break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
